{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "\n",
    "def REL_embedding(corpus_dict):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    RELSet = set()\n",
    "    for k,v in corpus_dict.items():\n",
    "        if v!=\"Other\":\n",
    "            rel=v.split(\"(\")\n",
    "            relation=rel[0]\n",
    "            RELSet.add(relation)\n",
    "    RELSet.add(\"O\")\n",
    "    li = list(RELSet) \n",
    "    trans = le.fit_transform(li)\n",
    "    \n",
    "    print(trans)\n",
    "    reldict = {}\n",
    "    for i in range(len(li)):\n",
    "        reldict[li[i]] = trans[i]\n",
    "    return reldict\n",
    "# REL_embedding(corpus_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "\n",
    "def RELWithDir_embedding(corpus_dict):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    RELSet = set()\n",
    "    for k,v in corpus_dict.items():\n",
    "        if v!=\"Other\":\n",
    "            RELSet.add(v)\n",
    "    RELSet.add(\"O\")\n",
    "    li = list(RELSet) \n",
    "    trans = le.fit_transform(li)\n",
    "    \n",
    "    reldict = {}\n",
    "    for i in range(len(li)):\n",
    "        reldict[li[i]] = trans[i]\n",
    "    return reldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "\n",
    "def POS_embedding():\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    set1 = {\"CC\",\"CD\",\"DT\",\"EX\",\"FW\",\"IN\",\"JJ\",\"JJR\",\"JJS\",\"LS\",\"MD\",\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"PDT\",\"POS\",\"PRP\",\"PRP$\",\"RB\",\"RBR\",\"RBS\",\"RP\",\"TO\",\"UH\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\",\"WDT\",\"WP\",\"WP$\",\"WRB\"}\n",
    "    li = list(set1) \n",
    "    li1 = []\n",
    "    li1.append(li)\n",
    "    trans = le.fit_transform(li)\n",
    "\n",
    "    posEmbDict = {}\n",
    "    for i in range(len(li)):\n",
    "        posEmbDict[li[i]] = trans[i]\n",
    "    return posEmbDict\n",
    "# POS_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "\n",
    "def depParsingEmbedding():\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    set1 = {'ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp','-'}\n",
    "    li = list(set1) \n",
    "    li1 = []\n",
    "    li1.append(li)\n",
    "    trans = le.fit_transform(li)\n",
    "\n",
    "    depParseEmbDict = {}\n",
    "    for i in range(len(li)):\n",
    "        depParseEmbDict[li[i]] = trans[i]\n",
    "    return depParseEmbDict\n",
    "# depParsingEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def NERTagEmbedding():\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    set1 = {\"PERSON\",\"NORP\",\"FAC\",\"ORG\",\"GPE\",\"LOC\",\"PRODUCT\",\"EVENT\",\"WORK_OF_ART\",\"LAW\",\"LANGUAGE\",\"DATE\",\"TIME\",\"PERCENT\",\"MONEY\",\"QUANTITY\",\"ORDINAL\",\"CARDINAL\",\"NaN\",\"None\"}\n",
    "    li = list(set1) \n",
    "    li1 = []\n",
    "    li1.append(li)\n",
    "    trans = le.fit_transform(li)\n",
    "\n",
    "    nerTagEmbDict = {}\n",
    "    for i in range(len(li)):\n",
    "        nerTagEmbDict[li[i]] = trans[i]\n",
    "    return nerTagEmbDict\n",
    "# NERTagEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def find_hypernyms(word):\n",
    "    hypernyms=[]\n",
    "    for ss in wn.synsets('green'):\n",
    "        for hyper in ss.hypernyms():\n",
    "            hypernyms.append(hyper.name())\n",
    "    return (hypernyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet as wn\n",
    "def Corpus_dictFormation(f):\n",
    "    d=(f.read())\n",
    "    data=d.split(\"\\n\\n\\n\")\n",
    "    data=data[:len(data)-1]\n",
    "    import re\n",
    "    for i in range(len(data)):\n",
    "        data[i]=re.sub(r\"(\\d+\\t)\",\" \",data[i])\n",
    "        data[i]=re.sub('\"',' ',data[i])\n",
    "\n",
    "    corpus_dict={}\n",
    "    for i in range(len(data)):\n",
    "    #     print(i)\n",
    "        temp=data[i].split(\"\\n\")\n",
    "\n",
    "        corpus_dict[temp[0]]=temp[1]\n",
    "    return corpus_dict\n",
    "\n",
    "def Corpus_Reading(corpus_dict):\n",
    "\n",
    "    ind=0\n",
    "    MainDict1={}\n",
    "    relList = []\n",
    "    dirList = []\n",
    "    wholeRelList = []\n",
    "    for k,v in corpus_dict.items():\n",
    "        wholeRelList.append(v)\n",
    "        if ind not in MainDict1.keys():\n",
    "            MainDict1[ind]={'token':{},'POSTAG':{},'NER':{},'DEPPARSE':{},'Direction':\"\",\"REL\":\"\",\"e1Start\":0,\"e1End\":0,\"e2Start\":0,\"e2End\":0}\n",
    "            if v!=\"Other\":\n",
    "                rel=v.split(\"(\")\n",
    "                rel1=rel[0]\n",
    "                relList.append(rel1)\n",
    "                dire=rel[1].split(\",\")\n",
    "                if dire[0]==\"e1\":\n",
    "                    MainDict1[ind][\"Direction\"]=\"+1\"\n",
    "                    MainDict1[ind][\"REL\"]=rel1\n",
    "                    dirList.append(1)\n",
    "                else:\n",
    "                    MainDict1[ind][\"Direction\"]=\"-1\"\n",
    "                    MainDict1[ind][\"REL\"]=rel1\n",
    "                    dirList.append(-1)\n",
    "            else:\n",
    "                MainDict1[ind][\"Direction\"]=\"0\"\n",
    "                MainDict1[ind][\"REL\"]=\"O\"\n",
    "                relList.append(v)\n",
    "                dirList.append(0)\n",
    "\n",
    "\n",
    "            tempSent=k\n",
    "            tempSent=re.sub('<e1>|</e1>|<e2>|</e2>|\"',\"\",tempSent).lstrip()\n",
    "            tempSent=re.sub('-|/',\" \",tempSent)\n",
    "\n",
    "            tokens=nltk.word_tokenize(tempSent)\n",
    "            for i in range(len(tokens)):\n",
    "                MainDict1[ind][\"token\"][i]=tokens[i]\n",
    "\n",
    "            nlpdoc = nlp(tempSent)\n",
    "            for nd in nlpdoc:\n",
    "        #         print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop)\n",
    "                if nd.text.isspace()==False and nd.text in tokens:\n",
    "                    MainDict1[ind]['DEPPARSE'][tokens.index(nd.text)]=nd.dep_\n",
    "\n",
    "\n",
    "            e1=re.findall(\"<e1>.*?</e1>\",k)\n",
    "            e2=re.findall(\"<e2>.*?</e2>\",k)\n",
    "            e1=re.sub(\"<e1>|</e1>\",\"\",e1[0])\n",
    "      \n",
    "            e1=re.sub(r\"([a-z])\\-([a-z])\", r\"\\1 \\2\", e1 , 0, re.IGNORECASE)\n",
    "            e2=re.sub(\"<e2>|</e2>\",\"\",e2[0])\n",
    "       \n",
    "            e2=re.sub(r\"([a-z])\\-([a-z])\", r\"\\1 \\2\", e2 , 0, re.IGNORECASE)\n",
    "            e1_list=e1.split(\" \")\n",
    "            e2_list=e2.split(\" \")\n",
    "    \n",
    "            MainDict1[ind][\"e1Start\"]=tokens.index(e1_list[0])\n",
    "            MainDict1[ind][\"e1End\"]=tokens.index(e1_list[-1])\n",
    "            MainDict1[ind][\"e2Start\"]=tokens.index(e2_list[0])\n",
    "            MainDict1[ind][\"e2End\"]=tokens.index(e2_list[-1])\n",
    "        \n",
    "            posTags=nltk.pos_tag(tokens)\n",
    "            for i in range(len(tokens)):\n",
    "                MainDict1[ind][\"POSTAG\"][i]=posTags[i][1]\n",
    "\n",
    "\n",
    "    \n",
    "            spacy_sent1=\" \".join(e1_list)\n",
    "            spacy_sent2=\" \".join(e2_list)\n",
    "   \n",
    "            doc1=nlp(spacy_sent1)\n",
    "            doc2=nlp(spacy_sent2)\n",
    "   \n",
    "            l1,l2=0,0\n",
    "            for i in tokens:\n",
    "                    MainDict1[ind]['NER'][tokens.index(i)]=\"None\"\n",
    "\n",
    "            for e1 in doc1.ents:\n",
    "                l1=e1.label_\n",
    "\n",
    "            for e2 in doc2.ents:\n",
    "                l2=e2.label_\n",
    "\n",
    "\n",
    "            for i in tokens:\n",
    "                if i in e1_list and MainDict1[ind]['NER'][tokens.index(i)]==\"None\":\n",
    "                    if l1==0:\n",
    "                        MainDict1[ind]['NER'][tokens.index(i)]=\"NaN\"\n",
    "                    else:\n",
    "                        MainDict1[ind]['NER'][tokens.index(i)]=l1\n",
    "                elif i in e2_list and MainDict1[ind]['NER'][tokens.index(i)]==\"None\":\n",
    "                    if l2==0:\n",
    "                        MainDict1[ind]['NER'][tokens.index(i)]=\"NaN\"\n",
    "                    else:\n",
    "                        MainDict1[ind]['NER'][tokens.index(i)]=l2\n",
    "\n",
    "            tempSent=\"\"  \n",
    "            ind+=1\n",
    "    MainDict2={}\n",
    "    ind=0\n",
    "\n",
    "    for k,v in corpus_dict.items():\n",
    "        if ind not in MainDict2.keys():\n",
    "            MainDict2[ind]={'token':[],'POSTAG':[],'NER':[],'DEPPARSE':[],'Direction':\"\",\"REL\":\"\",\"e1Start\":0,\"e1End\":0,\"e2Start\":0,\"e2End\":0,\"lemma\":[]}\n",
    "\n",
    "            for k1,v1 in MainDict1[ind]['token'].items():\n",
    "                 MainDict2[ind]['token'].append(v1)\n",
    "\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            for i in MainDict2[ind]['token']:\n",
    "                MainDict2[ind]['lemma'].append(lemmatizer.lemmatize(i))\n",
    "                \n",
    "#             for i in MainDict2[ind]['token']:\n",
    "#                 tempHyper=find_hypernyms(i) \n",
    "#                 MainDict2[ind]['hypernyms'].append(tempHyper)\n",
    "                \n",
    "            for k1,v1 in MainDict1[ind]['POSTAG'].items():\n",
    "                MainDict2[ind]['POSTAG'].append(v1)\n",
    "            for k1,v1 in MainDict1[ind]['NER'].items():\n",
    "                MainDict2[ind]['NER'].append(v1)\n",
    "            for k1,v1 in MainDict1[ind]['DEPPARSE'].items():\n",
    "                MainDict2[ind]['DEPPARSE'].append(v1)\n",
    "            MainDict2[ind]['REL']=MainDict1[ind]['REL']\n",
    "\n",
    "            MainDict2[ind]['Direction']=int(MainDict1[ind]['Direction'])\n",
    "            MainDict2[ind]['e1Start']=int(MainDict1[ind]['e1Start'])\n",
    "            MainDict2[ind]['e1End']=int(MainDict1[ind]['e1End'])\n",
    "            MainDict2[ind]['e2Start']=int(MainDict1[ind]['e2Start'])\n",
    "            MainDict2[ind]['e2End']=int(MainDict1[ind]['e2End'])\n",
    "            ind+=1\n",
    "            \n",
    "    MainDictEmbedding={}\n",
    "    ind=0\n",
    "    nerTagEmbDict=NERTagEmbedding()\n",
    "    depParseEmbDict=depParsingEmbedding()\n",
    "    posEmbDict=POS_embedding()\n",
    "    reldict=REL_embedding(corpus_dict)\n",
    "\n",
    "    for k,v in corpus_dict.items():\n",
    "        if ind not in MainDictEmbedding.keys():\n",
    "            MainDictEmbedding[ind]={'token':[],'POSTAG':[],'NER':[],'DEPPARSE':[],'Direction':\"\",\"REL\":\"\",\"e1Start\":0,\"e1End\":0,\"e2Start\":0,\"e2End\":0,\"lemma\":[]}\n",
    "\n",
    "            tempSent=k\n",
    "            tempSent=re.sub('<e1>|</e1>|<e2>|</e2>|\"',\"\",tempSent).lstrip()\n",
    "            tempSent=re.sub('-|/',\" \",tempSent)\n",
    "\n",
    "            tokens=nltk.word_tokenize(tempSent)\n",
    "            for i in range(len(tokens)):\n",
    "                MainDict1[ind][\"token\"][i]=tokens[i]\n",
    "            tokenli=[]\n",
    "            tokenli.append(tokens)\n",
    "            model = Word2Vec(tokenli, min_count=1,size=1)\n",
    "            words = list(model.wv.vocab)\n",
    "            for i in words:\n",
    "                if i in tokens:\n",
    "                    (MainDictEmbedding[ind]['token']).append(int(model[i][0]*1000))\n",
    "            MainDictEmbedding[ind]['token']=np.std(MainDictEmbedding[ind]['token'])\n",
    "            \n",
    "#             hyperEmb=MainDict2[ind]['hypernyms']\n",
    "#             model2 = Word2Vec(hyperEmb, min_count=1,size=1)\n",
    "#             words2 = list(model2.wv.vocab)\n",
    "#             for i in words2:\n",
    "#                 if i in hyperEmb:\n",
    "#                     (MainDictEmbedding[ind]['hypernyms']).append(int(np.mean(model2[i][0]*1000)))\n",
    "#             MainDictEmbedding[ind]['hypernyms']=np.mean(MainDictEmbedding[ind]['hypernyms'])\n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "            tempLemma=[]\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            for i in tokens:\n",
    "                tempLemma.append(lemmatizer.lemmatize(i))\n",
    "            lemmaLi=[]\n",
    "            lemmaLi.append(tempLemma)\n",
    "            model1 = Word2Vec(lemmaLi, min_count=1,size=1)\n",
    "            words1 = list(model1.wv.vocab)\n",
    "            for i in words1:\n",
    "                if i in tempLemma:\n",
    "                    (MainDictEmbedding[ind]['lemma']).append(int(model1[i][0]*1000))\n",
    "            MainDictEmbedding[ind]['lemma']=np.std(MainDictEmbedding[ind]['lemma'])\n",
    "            \n",
    "            \n",
    "\n",
    "            for k1,v1 in MainDict1[ind]['POSTAG'].items():\n",
    "                if v1 in posEmbDict.keys():\n",
    "                    (MainDictEmbedding[ind]['POSTAG']).append(posEmbDict[v1])\n",
    "                else:\n",
    "                    (MainDictEmbedding[ind]['POSTAG']).append(0)\n",
    "            MainDictEmbedding[ind]['POSTAG']=np.std(MainDictEmbedding[ind]['POSTAG'])\n",
    "\n",
    "            for k1,v1 in MainDict1[ind]['NER'].items():\n",
    "                if v1 in nerTagEmbDict.keys():\n",
    "                    (MainDictEmbedding[ind]['NER']).append(nerTagEmbDict[v1])\n",
    "                else:\n",
    "                    (MainDictEmbedding[ind]['NER']).append(0)\n",
    "            MainDictEmbedding[ind]['NER']=np.std(MainDictEmbedding[ind]['NER'])\n",
    "\n",
    "            for k1,v1 in MainDict1[ind]['DEPPARSE'].items():\n",
    "                if v1 in depParseEmbDict.keys():\n",
    "                    (MainDictEmbedding[ind]['DEPPARSE']).append(depParseEmbDict[v1])\n",
    "                else:\n",
    "                    (MainDictEmbedding[ind]['DEPPARSE']).append(0)\n",
    "            MainDictEmbedding[ind]['DEPPARSE']=np.std(MainDictEmbedding[ind]['DEPPARSE'])        \n",
    "\n",
    "            if MainDict1[ind]['REL'] in reldict:\n",
    "                MainDictEmbedding[ind]['REL']=reldict[MainDict1[ind]['REL']]\n",
    "\n",
    "            MainDictEmbedding[ind]['Direction']=int(MainDict1[ind]['Direction'])\n",
    "            MainDictEmbedding[ind]['e1Start']=int(MainDict1[ind]['e1Start'])\n",
    "            MainDictEmbedding[ind]['e1End']=int(MainDict1[ind]['e1End'])\n",
    "            MainDictEmbedding[ind]['e2Start']=int(MainDict1[ind]['e2Start'])\n",
    "            MainDictEmbedding[ind]['e2End']=int(MainDict1[ind]['e2End'])\n",
    "            ind+=1\n",
    "            \n",
    "    \n",
    "    df= pd.DataFrame.from_dict(MainDictEmbedding)\n",
    "\n",
    "    return(df,relList,dirList,wholeRelList)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 3 6 1 7 5 8 9 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prayaas\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:181: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\prayaas\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:205: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 3 6 1 7 5 8 9 4]\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import nltk\n",
    "f2=open(r\"test_data1.txt\")\n",
    "f1=open(r\"train_data1.txt\")\n",
    "corpus_dict1=Corpus_dictFormation(f1)\n",
    "corpus_dict2=Corpus_dictFormation(f2)\n",
    "\n",
    "df1,relList1,dirList1,wholeRelList1=Corpus_Reading(corpus_dict1)\n",
    "df2,relList2,dirList2,wholeRelList2=Corpus_Reading(corpus_dict2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prayaas\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\prayaas\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29970544918998526\n",
      "[[7.]\n",
      " [9.]\n",
      " [5.]\n",
      " ...\n",
      " [9.]\n",
      " [1.]\n",
      " [9.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "\n",
    "\n",
    "Train_X = np.asarray(df1.loc[['POSTAG', 'NER', 'DEPPARSE','Direction','lemma']])\n",
    "Train_Y = np.atleast_2d(np.asarray(df1.loc['REL']))\n",
    "Train_X=Train_X.T\n",
    "Train_Y=Train_Y.T\n",
    "# Train_Y=Train_Y.astype('int')\n",
    "\n",
    "Test_X = np.asarray(df2.loc[['POSTAG', 'NER', 'DEPPARSE','Direction','lemma']])\n",
    "Test_Y = np.atleast_2d(np.asarray(df2.loc['REL']))\n",
    "Test_X=Test_X.T\n",
    "Test_Y=Test_Y.T\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(Train_X,Train_Y)\n",
    "\n",
    "y_predict=np.atleast_2d(clf.predict(Test_X))\n",
    "# print(l)\n",
    "# print(y_predict)\n",
    "\n",
    "# print(clf.score(l,Test_Y.T))\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(Test_Y, y_predict.T))\n",
    "print(Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 3 6 1 7 5 8 9 4]\n",
      "2716\n"
     ]
    }
   ],
   "source": [
    "relDict = REL_embedding(corpus_dict1)\n",
    "count =0\n",
    "for v1 in y_predict[0]:\n",
    "    for k,v in relDict.items():\n",
    "        if v1 == v:\n",
    "#             print(k)\n",
    "            count +=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "relList2Encode =[]\n",
    "for l in relList2:\n",
    "    if l in relDict.keys():\n",
    "        relList2Encode.append(relDict[l])\n",
    "    else:\n",
    "        relList2Encode.append(10)\n",
    "        \n",
    "# print(relList2Encode)\n",
    "print(len(relList2Encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20655375552282768"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(relList2Encode, y_predict.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prayaas\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\prayaas\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "Train_X1 = np.asarray(df1.loc[['POSTAG', 'NER', 'DEPPARSE','REL','lemma']])\n",
    "Train_Y1 = np.atleast_2d(np.asarray(df1.loc['Direction']))\n",
    "Train_X1=Train_X1.T\n",
    "Train_Y1=Train_Y1.T\n",
    "\n",
    "\n",
    "Test_X1 = np.asarray(df2.loc[['POSTAG', 'NER', 'DEPPARSE','REL','lemma']])\n",
    "Test_Y1 = np.atleast_2d(np.asarray(df2.loc['Direction']))\n",
    "Test_X1=Test_X1.T\n",
    "Test_Y1=Test_Y1.T\n",
    "\n",
    "clf1 = svm.SVC()\n",
    "clf1.fit(Train_X1,Train_Y1)\n",
    "\n",
    "y_predict1=np.atleast_2d(clf1.predict(Test_X1))\n",
    "# print(l)\n",
    "print(y_predict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6649484536082474\n",
      "0.164580265095729\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(dirList2, y_predict1.T))\n",
    "count=0\n",
    "predictedRelList = []\n",
    "for v1 in range(len(y_predict[0])):\n",
    "    for k,v in relDict.items():\n",
    "        if y_predict[0][v1] == v and y_predict1[0][v1] == 1:\n",
    "            predictedRelList.append(str(k+\"(e1,e2)\"))\n",
    "            \n",
    "        elif y_predict[0][v1] == v and y_predict1[0][v1] == -1:\n",
    "            predictedRelList.append(str(k+\"(e2,e1)\"))\n",
    "            \n",
    "        elif y_predict[0][v1] == v and y_predict1[0][v1] == 0:\n",
    "            predictedRelList.append(k)\n",
    "            \n",
    "\n",
    "        \n",
    "for i in range(len(predictedRelList)):\n",
    "    if(predictedRelList[i] == wholeRelList2[i]):\n",
    "        count +=1\n",
    "print(count/len(predictedRelList))        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of               0        1        2      3        4        5        6     \\\n",
      "token      39.3529 -35.3125 -54.2667 -149.5 -29.8125  14.3846  7.96667   \n",
      "POSTAG     9.82353  10.5625   9.8125  6.375  8.52381       14  8.48649   \n",
      "NER        10.8824   10.875  10.8667  10.75   10.875  10.8462  10.9333   \n",
      "DEPPARSE   22.7647   25.625     25.8  24.75       25  24.9231     21.1   \n",
      "Direction       -1        0       -1      0        1        0       -1   \n",
      "REL              1        8        5      8        6        8        0   \n",
      "e1Start         12        1        1      2        1        4        7   \n",
      "e1End           12        1        1      2        1        4        7   \n",
      "e2Start         15        9        7      6        2       10       19   \n",
      "e2End           15        9        7      6        2       10       19   \n",
      "lemma           41    -43.5 -7.73333 -149.5 -29.8125  14.3846    -37.1   \n",
      "hypernyms       []       []       []     []       []       []       []   \n",
      "\n",
      "             7        8        9     ...     3809     3810     3811     3812  \\\n",
      "token      169.25  1.05882  7.40909  ...  80.8333     61.8  8.84615 -23.3182   \n",
      "POSTAG     16.625     9.75     9.12  ...  13.6667  10.9375  9.07547    10.56   \n",
      "NER         10.75  10.8235  10.9091  ...  10.8333  10.7333  10.9487  10.8636   \n",
      "DEPPARSE    21.75  21.9412  27.0909  ...  17.4167  23.4667  21.2821  22.5455   \n",
      "Direction       1        1        1  ...        0        1       -1        1   \n",
      "REL             3        2        3  ...        8        3        0        7   \n",
      "e1Start         0        1       12  ...        8        1       42        1   \n",
      "e1End           0        1       12  ...        8        2       42        1   \n",
      "e2Start         6        6       20  ...       10        7       49        4   \n",
      "e2End           6        7       20  ...       10        8       49        5   \n",
      "lemma      169.25  30.2941  18.5455  ...  118.083 -7.66667 -10.6154  7.45455   \n",
      "hypernyms      []       []       []  ...       []       []       []       []   \n",
      "\n",
      "              3813  3814  3815     3816     3817     3818  \n",
      "token     -37.9412  61.2  -4.3  16.8571 -20.8571 -8.83333  \n",
      "POSTAG         9.4   9.7   7.3  8.07692  10.7273     8.64  \n",
      "NER        10.8824  10.7  10.8  10.9048  11.3333  10.9167  \n",
      "DEPPARSE   21.8824  23.5  23.3  26.4762  22.8571       23  \n",
      "Direction        0     1     0        1       -1       -1  \n",
      "REL              8     2     8        1        0        6  \n",
      "e1Start          2     3     4       11       14       12  \n",
      "e1End            2     3     4       11       14       12  \n",
      "e2Start         10     7     8       14       19       15  \n",
      "e2End           10     8     8       14       20       15  \n",
      "lemma     -34.5625  61.6  -4.3  16.8571  1.90476  16.4583  \n",
      "hypernyms       []    []    []       []       []       []  \n",
      "\n",
      "[12 rows x 3819 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(df1.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['chromatic_color.n.01', 'tract.n.01', 'environmentalist.n.01', 'site.n.01', 'vegetable.n.01', 'ketamine.n.01', 'discolor.v.03']]\n"
     ]
    }
   ],
   "source": [
    "# x=[]\n",
    "# for i in ['dog']:\n",
    "#     y=find_hypernyms(i) \n",
    "#     x.append(y)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['chromatic_color.n.01', 'tract.n.01', 'environmentalist.n.01', 'site.n.01', 'vegetable.n.01', 'ketamine.n.01', 'discolor.v.03']]\n",
      "[['chromatic_color.n.01', 'tract.n.01', 'environmentalist.n.01', 'site.n.01', 'vegetable.n.01', 'ketamine.n.01', 'discolor.v.03']]\n",
      "[['chromatic_color.n.01', 'tract.n.01', 'environmentalist.n.01', 'site.n.01', 'vegetable.n.01', 'ketamine.n.01', 'discolor.v.03']]\n",
      "[['chromatic_color.n.01', 'tract.n.01', 'environmentalist.n.01', 'site.n.01', 'vegetable.n.01', 'ketamine.n.01', 'discolor.v.03']]\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "# hyperEmb1=x\n",
    "# res=[]\n",
    "\n",
    "# # print(hyperEmb1)\n",
    "# for i in hyperEmb1:\n",
    "#     some=[]\n",
    "    \n",
    "#     some.append(i)\n",
    "#     print(some)\n",
    "#     modelh = Word2Vec(some, min_count=1,size=1)\n",
    "#     wordsh = list(modelh.wv.vocab)\n",
    "#     for i in wordsh:\n",
    "#         if i in hyperEmb1:\n",
    "#             res.append(int(np.mean(modelh[i][0]*1000)))\n",
    "# ans=np.mean(res)\n",
    "# print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1800441826215022\n"
     ]
    }
   ],
   "source": [
    "count1=0\n",
    "predictedRelationList = []\n",
    "for v1 in range(len(y_predict[0])):\n",
    "    for k,v in relDict.items():\n",
    "        if y_predict[0][v1] == v:\n",
    "            predictedRelationList.append((k))\n",
    "            \n",
    "        elif y_predict[0][v1] == v :\n",
    "            predictedRelationList.append(k)\n",
    "            \n",
    "        elif y_predict[0][v1] == v:\n",
    "            predictedRelationList.append(k)\n",
    "            \n",
    "onlyrel=[]\n",
    "for i in wholeRelList2:\n",
    "    if i!=\"O\":\n",
    "        onlyrel.append(i.split(\"(\")[0])\n",
    "    else:\n",
    "        onlyrel.append(\"O\")\n",
    "        \n",
    "\n",
    "for i in range(len(wholeRelList2)):\n",
    "    if onlyrel[i]==predictedRelationList[i]:\n",
    "        count1+=1\n",
    "print(count1/len(predictedRelationList))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
